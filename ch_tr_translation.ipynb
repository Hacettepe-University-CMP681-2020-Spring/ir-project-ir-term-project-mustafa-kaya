{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"ch_tr.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "  w = w.strip()\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "num_examples = None\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2111 2111\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tensor), len(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "17 ----> хевел\n",
      "1049 ----> пехре\n",
      "589 ----> ашатса\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "31 ----> gunes\n",
      "233 ----> bakar\n",
      "234 ----> sıcacık\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor[2])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 8]), TensorShape([64, 8]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 8, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 2689)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    \n",
    "    for t in range(1, targ.shape[1]):\n",
    "      \n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "     \n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.9793\n",
      "Epoch 1 Loss 3.4269\n",
      "Time taken for 1 epoch 26.16159224510193 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.8642\n",
      "Epoch 2 Loss 2.9464\n",
      "Time taken for 1 epoch 3.737898826599121 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.0124\n",
      "Epoch 3 Loss 2.7762\n",
      "Time taken for 1 epoch 2.243464469909668 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.6794\n",
      "Epoch 4 Loss 2.6646\n",
      "Time taken for 1 epoch 3.7371256351470947 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.5122\n",
      "Epoch 5 Loss 2.5933\n",
      "Time taken for 1 epoch 2.239417552947998 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.4847\n",
      "Epoch 6 Loss 2.5196\n",
      "Time taken for 1 epoch 3.6996030807495117 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.4982\n",
      "Epoch 7 Loss 2.4502\n",
      "Time taken for 1 epoch 2.2428369522094727 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.1911\n",
      "Epoch 8 Loss 2.3681\n",
      "Time taken for 1 epoch 3.7125003337860107 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.2107\n",
      "Epoch 9 Loss 2.2818\n",
      "Time taken for 1 epoch 2.2416293621063232 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.0310\n",
      "Epoch 10 Loss 2.1988\n",
      "Time taken for 1 epoch 4.148504972457886 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.0499\n",
      "Epoch 11 Loss 2.1053\n",
      "Time taken for 1 epoch 2.2590973377227783 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.9395\n",
      "Epoch 12 Loss 1.9953\n",
      "Time taken for 1 epoch 3.7033498287200928 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.8897\n",
      "Epoch 13 Loss 1.9183\n",
      "Time taken for 1 epoch 2.2422990798950195 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.7356\n",
      "Epoch 14 Loss 1.7857\n",
      "Time taken for 1 epoch 3.7202062606811523 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.5844\n",
      "Epoch 15 Loss 1.6511\n",
      "Time taken for 1 epoch 2.23620343208313 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.3866\n",
      "Epoch 16 Loss 1.5161\n",
      "Time taken for 1 epoch 4.165785074234009 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2713\n",
      "Epoch 17 Loss 1.3672\n",
      "Time taken for 1 epoch 2.2509429454803467 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1162\n",
      "Epoch 18 Loss 1.2392\n",
      "Time taken for 1 epoch 3.7067463397979736 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.9839\n",
      "Epoch 19 Loss 1.0924\n",
      "Time taken for 1 epoch 2.252063512802124 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.8474\n",
      "Epoch 20 Loss 0.9501\n",
      "Time taken for 1 epoch 3.660945177078247 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.7295\n",
      "Epoch 21 Loss 0.8369\n",
      "Time taken for 1 epoch 2.2467739582061768 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.6659\n",
      "Epoch 22 Loss 0.7335\n",
      "Time taken for 1 epoch 3.71382999420166 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.5195\n",
      "Epoch 23 Loss 0.6230\n",
      "Time taken for 1 epoch 2.2440025806427 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4955\n",
      "Epoch 24 Loss 0.5487\n",
      "Time taken for 1 epoch 3.802478313446045 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.3987\n",
      "Epoch 25 Loss 0.4760\n",
      "Time taken for 1 epoch 2.2529759407043457 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.3431\n",
      "Epoch 26 Loss 0.4030\n",
      "Time taken for 1 epoch 3.6855762004852295 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.2847\n",
      "Epoch 27 Loss 0.3364\n",
      "Time taken for 1 epoch 2.2553341388702393 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.2422\n",
      "Epoch 28 Loss 0.2956\n",
      "Time taken for 1 epoch 3.706395387649536 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.2016\n",
      "Epoch 29 Loss 0.2359\n",
      "Time taken for 1 epoch 2.248914957046509 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.1644\n",
      "Epoch 30 Loss 0.1808\n",
      "Time taken for 1 epoch 3.6577401161193848 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0810\n",
      "Epoch 31 Loss 0.1376\n",
      "Time taken for 1 epoch 2.248847723007202 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0704\n",
      "Epoch 32 Loss 0.1049\n",
      "Time taken for 1 epoch 4.305775880813599 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0531\n",
      "Epoch 33 Loss 0.0769\n",
      "Time taken for 1 epoch 2.252976894378662 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0441\n",
      "Epoch 34 Loss 0.0574\n",
      "Time taken for 1 epoch 3.736482858657837 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0391\n",
      "Epoch 35 Loss 0.0449\n",
      "Time taken for 1 epoch 2.2636454105377197 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0217\n",
      "Epoch 36 Loss 0.0363\n",
      "Time taken for 1 epoch 3.7434051036834717 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0180\n",
      "Epoch 37 Loss 0.0329\n",
      "Time taken for 1 epoch 2.2523293495178223 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0287\n",
      "Epoch 38 Loss 0.0318\n",
      "Time taken for 1 epoch 3.8088431358337402 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0287\n",
      "Epoch 39 Loss 0.0412\n",
      "Time taken for 1 epoch 2.2366421222686768 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0700\n",
      "Epoch 40 Loss 0.0395\n",
      "Time taken for 1 epoch 3.720608949661255 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0311\n",
      "Epoch 41 Loss 0.0374\n",
      "Time taken for 1 epoch 2.2762527465820312 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0186\n",
      "Epoch 42 Loss 0.0438\n",
      "Time taken for 1 epoch 3.7274131774902344 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0254\n",
      "Epoch 43 Loss 0.0354\n",
      "Time taken for 1 epoch 2.2667112350463867 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0198\n",
      "Epoch 44 Loss 0.0276\n",
      "Time taken for 1 epoch 3.744417428970337 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0127\n",
      "Epoch 45 Loss 0.0239\n",
      "Time taken for 1 epoch 2.2509775161743164 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0086\n",
      "Epoch 46 Loss 0.0189\n",
      "Time taken for 1 epoch 4.548038482666016 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0272\n",
      "Epoch 47 Loss 0.0184\n",
      "Time taken for 1 epoch 2.2465336322784424 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0109\n",
      "Epoch 48 Loss 0.0153\n",
      "Time taken for 1 epoch 3.7122879028320312 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0209\n",
      "Epoch 49 Loss 0.0147\n",
      "Time taken for 1 epoch 2.2449982166290283 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0062\n",
      "Epoch 50 Loss 0.0127\n",
      "Time taken for 1 epoch 3.730015993118286 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0043\n",
      "Epoch 51 Loss 0.0119\n",
      "Time taken for 1 epoch 2.253974437713623 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0136\n",
      "Epoch 52 Loss 0.0124\n",
      "Time taken for 1 epoch 3.7903411388397217 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0287\n",
      "Epoch 53 Loss 0.0127\n",
      "Time taken for 1 epoch 2.2506837844848633 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0045\n",
      "Epoch 54 Loss 0.0115\n",
      "Time taken for 1 epoch 3.706275463104248 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0036\n",
      "Epoch 55 Loss 0.0109\n",
      "Time taken for 1 epoch 2.249926805496216 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0041\n",
      "Epoch 56 Loss 0.0108\n",
      "Time taken for 1 epoch 3.6636950969696045 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0040\n",
      "Epoch 57 Loss 0.0106\n",
      "Time taken for 1 epoch 2.2563588619232178 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0111\n",
      "Epoch 58 Loss 0.0106\n",
      "Time taken for 1 epoch 3.695859909057617 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0037\n",
      "Epoch 59 Loss 0.0094\n",
      "Time taken for 1 epoch 2.2475454807281494 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0041\n",
      "Epoch 60 Loss 0.0098\n",
      "Time taken for 1 epoch 4.279418706893921 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0024\n",
      "Epoch 61 Loss 0.0102\n",
      "Time taken for 1 epoch 2.254464864730835 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0073\n",
      "Epoch 62 Loss 0.0100\n",
      "Time taken for 1 epoch 3.683518648147583 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0040\n",
      "Epoch 63 Loss 0.0096\n",
      "Time taken for 1 epoch 2.249986171722412 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0032\n",
      "Epoch 64 Loss 0.0098\n",
      "Time taken for 1 epoch 3.7026450634002686 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0098\n",
      "Epoch 65 Loss 0.0089\n",
      "Time taken for 1 epoch 2.251474380493164 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0020\n",
      "Epoch 66 Loss 0.0089\n",
      "Time taken for 1 epoch 3.713414430618286 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0071\n",
      "Epoch 67 Loss 0.0094\n",
      "Time taken for 1 epoch 2.2669408321380615 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0078\n",
      "Epoch 68 Loss 0.0088\n",
      "Time taken for 1 epoch 3.7790520191192627 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0018\n",
      "Epoch 69 Loss 0.0097\n",
      "Time taken for 1 epoch 2.256134510040283 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0021\n",
      "Epoch 70 Loss 0.0089\n",
      "Time taken for 1 epoch 3.6483328342437744 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0048\n",
      "Epoch 71 Loss 0.0086\n",
      "Time taken for 1 epoch 2.25260853767395 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0019\n",
      "Epoch 72 Loss 0.0082\n",
      "Time taken for 1 epoch 3.69026255607605 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0060\n",
      "Epoch 73 Loss 0.0084\n",
      "Time taken for 1 epoch 2.250415325164795 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0022\n",
      "Epoch 74 Loss 0.0098\n",
      "Time taken for 1 epoch 3.76684832572937 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0111\n",
      "Epoch 75 Loss 0.0224\n",
      "Time taken for 1 epoch 2.254251718521118 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0357\n",
      "Epoch 76 Loss 0.0959\n",
      "Time taken for 1 epoch 3.690761089324951 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.1535\n",
      "Epoch 77 Loss 0.2301\n",
      "Time taken for 1 epoch 2.2489705085754395 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.1352\n",
      "Epoch 78 Loss 0.2546\n",
      "Time taken for 1 epoch 3.7027480602264404 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.1172\n",
      "Epoch 79 Loss 0.1804\n",
      "Time taken for 1 epoch 2.2544498443603516 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0993\n",
      "Epoch 80 Loss 0.1047\n",
      "Time taken for 1 epoch 3.67089581489563 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0337\n",
      "Epoch 81 Loss 0.0568\n",
      "Time taken for 1 epoch 2.2519803047180176 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0203\n",
      "Epoch 82 Loss 0.0317\n",
      "Time taken for 1 epoch 4.47995400428772 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0143\n",
      "Epoch 83 Loss 0.0228\n",
      "Time taken for 1 epoch 2.254955768585205 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0107\n",
      "Epoch 84 Loss 0.0155\n",
      "Time taken for 1 epoch 3.73915958404541 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0041\n",
      "Epoch 85 Loss 0.0121\n",
      "Time taken for 1 epoch 2.2629518508911133 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Loss 0.0099\n",
      "Time taken for 1 epoch 3.6894278526306152 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0112\n",
      "Epoch 87 Loss 0.0096\n",
      "Time taken for 1 epoch 2.2480556964874268 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0078\n",
      "Epoch 88 Loss 0.0097\n",
      "Time taken for 1 epoch 3.7644410133361816 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0012\n",
      "Epoch 89 Loss 0.0088\n",
      "Time taken for 1 epoch 2.249243974685669 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0054\n",
      "Epoch 90 Loss 0.0085\n",
      "Time taken for 1 epoch 3.7476425170898438 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0081\n",
      "Epoch 91 Loss 0.0086\n",
      "Time taken for 1 epoch 2.242006778717041 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0073\n",
      "Epoch 92 Loss 0.0080\n",
      "Time taken for 1 epoch 3.675633668899536 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0023\n",
      "Epoch 93 Loss 0.0079\n",
      "Time taken for 1 epoch 2.2470576763153076 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0032\n",
      "Epoch 94 Loss 0.0076\n",
      "Time taken for 1 epoch 3.6788971424102783 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0029\n",
      "Epoch 95 Loss 0.0069\n",
      "Time taken for 1 epoch 2.2450242042541504 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0053\n",
      "Epoch 96 Loss 0.0075\n",
      "Time taken for 1 epoch 4.134304046630859 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0020\n",
      "Epoch 97 Loss 0.0073\n",
      "Time taken for 1 epoch 2.2607007026672363 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0145\n",
      "Epoch 98 Loss 0.0072\n",
      "Time taken for 1 epoch 3.7150931358337402 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0089\n",
      "Epoch 99 Loss 0.0072\n",
      "Time taken for 1 epoch 2.263007164001465 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0009\n",
      "Epoch 100 Loss 0.0063\n",
      "Time taken for 1 epoch 3.6981966495513916 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0007\n",
      "Epoch 101 Loss 0.0071\n",
      "Time taken for 1 epoch 2.2463924884796143 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0079\n",
      "Epoch 102 Loss 0.0065\n",
      "Time taken for 1 epoch 3.71504807472229 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0062\n",
      "Epoch 103 Loss 0.0065\n",
      "Time taken for 1 epoch 2.2519774436950684 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0024\n",
      "Epoch 104 Loss 0.0072\n",
      "Time taken for 1 epoch 4.506864309310913 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0119\n",
      "Epoch 105 Loss 0.0072\n",
      "Time taken for 1 epoch 2.2530035972595215 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0019\n",
      "Epoch 106 Loss 0.0072\n",
      "Time taken for 1 epoch 3.6993658542633057 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0035\n",
      "Epoch 107 Loss 0.0069\n",
      "Time taken for 1 epoch 2.2499852180480957 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0171\n",
      "Epoch 108 Loss 0.0063\n",
      "Time taken for 1 epoch 3.703749179840088 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0066\n",
      "Epoch 109 Loss 0.0065\n",
      "Time taken for 1 epoch 2.2517662048339844 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0056\n",
      "Epoch 110 Loss 0.0066\n",
      "Time taken for 1 epoch 3.735159397125244 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0049\n",
      "Epoch 111 Loss 0.0064\n",
      "Time taken for 1 epoch 2.260870933532715 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0038\n",
      "Epoch 112 Loss 0.0063\n",
      "Time taken for 1 epoch 4.447268486022949 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0108\n",
      "Epoch 113 Loss 0.0070\n",
      "Time taken for 1 epoch 2.25337815284729 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0166\n",
      "Epoch 114 Loss 0.0065\n",
      "Time taken for 1 epoch 3.6977920532226562 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0060\n",
      "Epoch 115 Loss 0.0067\n",
      "Time taken for 1 epoch 2.253181219100952 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0013\n",
      "Epoch 116 Loss 0.0068\n",
      "Time taken for 1 epoch 3.714554786682129 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0057\n",
      "Epoch 117 Loss 0.0062\n",
      "Time taken for 1 epoch 2.2579288482666016 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0072\n",
      "Epoch 118 Loss 0.0061\n",
      "Time taken for 1 epoch 3.7855613231658936 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0070\n",
      "Epoch 119 Loss 0.0063\n",
      "Time taken for 1 epoch 2.252678394317627 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0005\n",
      "Epoch 120 Loss 0.0066\n",
      "Time taken for 1 epoch 3.715343475341797 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0031\n",
      "Epoch 121 Loss 0.0065\n",
      "Time taken for 1 epoch 2.2601265907287598 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0010\n",
      "Epoch 122 Loss 0.0066\n",
      "Time taken for 1 epoch 3.699435234069824 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0028\n",
      "Epoch 123 Loss 0.0061\n",
      "Time taken for 1 epoch 2.2469189167022705 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0041\n",
      "Epoch 124 Loss 0.0062\n",
      "Time taken for 1 epoch 3.739664077758789 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0077\n",
      "Epoch 125 Loss 0.0069\n",
      "Time taken for 1 epoch 2.2538979053497314 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0104\n",
      "Epoch 126 Loss 0.0067\n",
      "Time taken for 1 epoch 4.518654108047485 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0004\n",
      "Epoch 127 Loss 0.0061\n",
      "Time taken for 1 epoch 2.2710392475128174 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0045\n",
      "Epoch 128 Loss 0.0061\n",
      "Time taken for 1 epoch 3.689969301223755 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0026\n",
      "Epoch 129 Loss 0.0057\n",
      "Time taken for 1 epoch 2.2555177211761475 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0004\n",
      "Epoch 130 Loss 0.0058\n",
      "Time taken for 1 epoch 3.7083091735839844 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0031\n",
      "Epoch 131 Loss 0.0058\n",
      "Time taken for 1 epoch 2.252244472503662 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0022\n",
      "Epoch 132 Loss 0.0060\n",
      "Time taken for 1 epoch 3.82914137840271 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0031\n",
      "Epoch 133 Loss 0.0056\n",
      "Time taken for 1 epoch 2.249610185623169 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0049\n",
      "Epoch 134 Loss 0.0063\n",
      "Time taken for 1 epoch 3.737744092941284 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0028\n",
      "Epoch 135 Loss 0.0055\n",
      "Time taken for 1 epoch 2.2544496059417725 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0062\n",
      "Epoch 136 Loss 0.0077\n",
      "Time taken for 1 epoch 3.6890251636505127 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0011\n",
      "Epoch 137 Loss 0.0282\n",
      "Time taken for 1 epoch 2.254575252532959 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0395\n",
      "Epoch 138 Loss 0.0807\n",
      "Time taken for 1 epoch 3.700549602508545 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0990\n",
      "Epoch 139 Loss 0.1353\n",
      "Time taken for 1 epoch 2.25437593460083 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0950\n",
      "Epoch 140 Loss 0.1478\n",
      "Time taken for 1 epoch 4.267557621002197 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0930\n",
      "Epoch 141 Loss 0.1324\n",
      "Time taken for 1 epoch 2.2551369667053223 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.1099\n",
      "Epoch 142 Loss 0.0957\n",
      "Time taken for 1 epoch 3.703359603881836 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0370\n",
      "Epoch 143 Loss 0.0601\n",
      "Time taken for 1 epoch 2.253749132156372 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0270\n",
      "Epoch 144 Loss 0.0361\n",
      "Time taken for 1 epoch 3.7467236518859863 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0076\n",
      "Epoch 145 Loss 0.0192\n",
      "Time taken for 1 epoch 2.259199857711792 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0087\n",
      "Epoch 146 Loss 0.0147\n",
      "Time taken for 1 epoch 3.7044808864593506 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0147\n",
      "Epoch 147 Loss 0.0111\n",
      "Time taken for 1 epoch 2.254934549331665 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0015\n",
      "Epoch 148 Loss 0.0094\n",
      "Time taken for 1 epoch 4.569087028503418 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0036\n",
      "Epoch 149 Loss 0.0081\n",
      "Time taken for 1 epoch 2.250356674194336 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0048\n",
      "Epoch 150 Loss 0.0076\n",
      "Time taken for 1 epoch 3.680516481399536 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  \n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x26039057148>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> cyк шим манан араскала <end>\n",
      "Predicted translation: nasıl mutlu olurum <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Çyк шим манӑн ӑрӑскалӑ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> пуш уиахен веcенче <end>\n",
      "Predicted translation: mart ayı sonlarında <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Пуш уйӑхӗн вӗçӗнче')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> килче ыра cуркунне <end>\n",
      "Predicted translation: geldi guzel ilkbahar <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Килчӗ ырӑ çуркунне')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
